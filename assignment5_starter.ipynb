{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k=3, distance_metric='euclidean'):\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X.values if isinstance(X, pd.DataFrame) else X\n",
    "        self.y_train = y.values if isinstance(y, pd.Series) else y\n",
    "\n",
    "    def compute_distance(self, X1, X2):\n",
    "        if self.distance_metric == 'euclidean':\n",
    "            return np.sqrt(np.sum((X1 - X2)**2, axis=1))\n",
    "        elif self.distance_metric == 'manhattan':\n",
    "            return np.sum(np.abs(X1 - X2), axis=1)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = X.values if isinstance(X, pd.DataFrame) else X\n",
    "        probas = []\n",
    "        for sample in X:\n",
    "            distances = self.compute_distance(sample, self.X_train)\n",
    "            k_indices = np.argsort(distances)[:self.k]\n",
    "            k_nearest_labels = self.y_train[k_indices]\n",
    "            proba = np.sum(k_nearest_labels) / self.k\n",
    "            probas.append(proba)\n",
    "        return np.array(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from scipy import stats\n",
    "\n",
    "# Define data preprocessing function\n",
    "def preprocess_data(train_path, test_path):\n",
    "    \n",
    "    train_data = pd.read_csv(train_path)\n",
    "    test_data = pd.read_csv(test_path)\n",
    "\n",
    "    # TODO: Implement data preprocessing\n",
    "    # Handle categorical variables, scale features, etc.\n",
    "    train_data.drop(columns=['CustomerId', 'Surname', 'id'], inplace=True)\n",
    "    test_data.drop(columns=['CustomerId', 'Surname', 'id'], inplace=True)\n",
    "\n",
    "    train_data['HasCrCard'] = train_data['HasCrCard'].astype('object')\n",
    "    train_data['IsActiveMember'] = train_data['IsActiveMember'].astype('object')\n",
    "\n",
    "    test_data['HasCrCard'] = test_data['HasCrCard'].astype('object')\n",
    "    test_data['IsActiveMember'] = test_data['IsActiveMember'].astype('object')\n",
    "    \n",
    "    train_data = pd.get_dummies(train_data, columns=['Geography', 'Gender'], drop_first=True)\n",
    "    test_data = pd.get_dummies(test_data, columns=['Geography', 'Gender'], drop_first=True)\n",
    "\n",
    "    numerical_features = train_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    if 'Exited' in numerical_features:\n",
    "        numerical_features.remove('Exited')\n",
    "    \n",
    "    def detect_outliers(df, features, threshold=3):\n",
    "        outliers = np.zeros(df.shape[0])\n",
    "        for feature in numerical_features:\n",
    "            z_scores = np.abs(stats.zscore(df[feature]))\n",
    "            outliers += (z_scores > threshold).astype(int)\n",
    "        return outliers > 0\n",
    "\n",
    "    outliers = detect_outliers(train_data, numerical_features)\n",
    "    train_data = train_data[~outliers]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    train_data[numerical_features] = scaler.fit_transform(train_data[numerical_features])\n",
    "    test_data[numerical_features] = scaler.transform(test_data[numerical_features])\n",
    "\n",
    "    X = train_data.drop('Exited', axis=1)\n",
    "    X = X.astype('float')\n",
    "    y = train_data['Exited']\n",
    "    X_test = test_data\n",
    "\n",
    "    return X, y, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Define cross-validation function\n",
    "def cross_validate(X, y, knn, n_splits=5):\n",
    "\n",
    "    # TODO: Implement cross-validation\n",
    "    # Compute ROC AUC scores\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(X, y), 1):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred_proba = knn.predict(X_val)\n",
    "        \n",
    "        score = roc_auc_score(y_val, y_pred_proba)\n",
    "        scores.append(score)\n",
    "\n",
    "    mean_score = np.mean(scores)\n",
    "    print(f\"Mean ROC AUC: {mean_score}\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ROC AUC: 0.9140720881211382\n",
      "Cross-validation scores: [0.9047709484634938, 0.9128112769566723, 0.9144948179111314, 0.9209717132386623, 0.9173116840357307]\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "X, y, X_test = preprocess_data('train.csv', 'test.csv')\n",
    "\n",
    "# Create and evaluate model\n",
    "knn = KNN(k=20, distance_metric='euclidean')\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_validate(X, y, knn)\n",
    "\n",
    "print(\"Cross-validation scores:\", cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ROC AUC: 0.9007196349458138\n",
      "Mean ROC AUC: 0.9006050983467088\n",
      "Mean ROC AUC: 0.9140720881211382\n",
      "Mean ROC AUC: 0.9106210450438198\n",
      "Mean ROC AUC: 0.9179929735449992\n",
      "Mean ROC AUC: 0.9153873727072653\n",
      "Mean ROC AUC: 0.9181740216285267\n",
      "Mean ROC AUC: 0.9168007085247103\n",
      "Mean ROC AUC: 0.9179001374650386\n",
      "Mean ROC AUC: 0.9181502644759393\n",
      "Mean ROC AUC: 0.9178381571911857\n",
      "Mean ROC AUC: 0.918390008104532\n",
      "Mean ROC AUC: 0.9175086121734124\n",
      "Mean ROC AUC: 0.9181492745534632\n",
      "Mean ROC AUC: 0.9172424256174667\n",
      "Mean ROC AUC: 0.918240054281136\n",
      "Mean ROC AUC: 0.9168523947467395\n",
      "Mean ROC AUC: 0.9175092733548581\n",
      "Mean ROC AUC: 0.9167060363536421\n",
      "Mean ROC AUC: 0.9178428727043402\n",
      "Best hyperparameters: k=60, distance_metric=manhattan\n",
      "Best mean ROC AUC: 0.918390008104532\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning\n",
    "k_values = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "distance_metrics = ['euclidean', 'manhattan']\n",
    "\n",
    "best_k = 0\n",
    "best_metric = None\n",
    "best_score = 0\n",
    "\n",
    "for k in k_values:\n",
    "    for metric in distance_metrics:\n",
    "        knn = KNN(k=k, distance_metric=metric)\n",
    "        scores = cross_validate(X, y, knn)\n",
    "        mean_score = np.mean(scores)\n",
    "        \n",
    "        if mean_score > best_score:\n",
    "            best_score = mean_score\n",
    "            best_k = k\n",
    "            best_metric = metric\n",
    "\n",
    "print(f\"Best hyperparameters: k={best_k}, distance_metric={best_metric}\")\n",
    "print(f\"Best mean ROC AUC: {best_score:}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on full dataset with optimal hyperparameters and make predictions on test set\n",
    "knn = KNN(k=best_k, distance_metric=best_metric)\n",
    "knn.fit(X, y)\n",
    "test_predictions = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test predictions\n",
    "submission = pd.DataFrame({'id': pd.read_csv('test.csv')['id'], 'Exited': test_predictions})\n",
    "submission.to_csv('submissions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
